# OHLightLLM 总体架构文档

## 1. 系统整体架构图

### 1.1 三层架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                    OpenHarmony 应用层                        │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │  ArkTS UI    │  │   ArkTS API   │  │   Demo App   │      │
│  │  - OCR      │  │  - OHLightLLM │  │  - Chat      │      │
│  │  - Chat     │  │  - Inference  │  │  - Vision    │      │
│  └──────┬──────┘  └──────┬───────┘  └──────┬───────┘      │
└─────────┼─────────────────┼─────────────────┼───────────────┘
          │                 │                 │
          └─────────────────┴─────────────────┘
                            │
                            │ NAPI / FFI
                            │
┌───────────────────────────┴───────────────────────────────────┐
│              OHLightLLM Native 封装层                          │
│  ┌────────────────────────────────────────────────────────┐  │
│  │         MLLM Runtime (C++ Core)                       │  │
│  │  - Model Loading & Management                         │  │
│  │  - Inference Engine                                   │  │
│  │  - Memory Management                                  │  │
│  │  - Tensor Operations                                 │  │
│  └─────────┬──────────────────────────────────────────────┘  │
│            │                                                  │
│  ┌─────────┴──────────────────────────────────────────────┐  │
│  │         OpenHarmony Native API Bridge                  │  │
│  │  - NAPI (Native API) Interface                        │  │
│  │  - ArkTS ↔ C++ Data Conversion                        │  │
│  │  - Async Callback Management                          │  │
│  └─────────┬──────────────────────────────────────────────┘  │
└────────────┼──────────────────────────────────────────────────┘
             │
┌────────────┴──────────────────────────────────────────────────┐
│            OpenHarmony 系统服务层                              │
│  ┌────────────────────────────────────────────────────────┐  │
│  │         HiAI 引擎集成层                                │  │
│  │  - HiAI Engine API                                     │  │
│  │  - Model Compilation & Optimization                   │  │
│  │  - Hardware Acceleration (NPU)                        │  │
│  │  - Graph Execution                                    │  │
│  └─────────┬──────────────────────────────────────────────┘  │
│            │                                                  │
│  ┌─────────┴──────────────────────────────────────────────┐  │
│  │         NN Runtime (神经网络推理框架)                  │  │
│  │  - Neural Network Runtime API                         │  │
│  │  - Graph Execution                                    │  │
│  │  - Tensor Management                                  │  │
│  │  - Device Scheduling                                  │  │
│  └─────────┬──────────────────────────────────────────────┘  │
└────────────┼──────────────────────────────────────────────────┘
             │
┌────────────┴──────────────────────────────────────────────────┐
│              硬件抽象层 (HAL)                                  │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │   CPU    │  │   GPU     │  │   NPU     │  │  Memory  │     │
│  │  (Arm)   │  │ (Mali)    │  │ (HiAI)    │  │  Manager │     │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘     │
└───────────────────────────────────────────────────────────────┘
```

### 1.2 模块依赖关系

```
┌─────────────┐
│  Demo Apps  │
└──────┬──────┘
       │
       ▼
┌─────────────┐      ┌─────────────┐
│  ArkTS API  │◄─────┤  NAPI Bridge │
└─────────────┘      └──────┬──────┘
                             │
                             ▼
                    ┌─────────────────┐
                    │  MLLM Runtime   │
                    │  ┌───────────┐  │
                    │  │  Backend  │  │
                    │  │  Manager  │  │
                    │  └─────┬─────┘  │
                    └────────┼────────┘
                             │
        ┌────────────────────┼────────────────────┐
        │                    │                    │
        ▼                    ▼                    ▼
┌──────────────┐    ┌──────────────┐    ┌──────────────┐
│ HiAI Backend │    │ NN RT Backend │    │ CPU Backend  │
└──────┬───────┘    └──────┬───────┘    └──────┬───────┘
       │                   │                   │
       └───────────────────┴───────────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  Hardware     │
                    └──────────────┘
```

## 2. 各层职责说明

### 2.1 应用层（ArkTS）

**职责**：
- 用户界面展示和交互
- 业务逻辑处理
- 调用 Native 层推理接口
- 处理用户输入（文本、图像、语音）
- 展示推理结果

**技术栈**：
- ArkTS 语言
- ArkUI 框架
- @ohos.xxx 系统 API

**关键组件**：
- `OHLightLLM` 类：主要推理接口
- Demo 应用：OCR、对话、视觉理解

### 2.2 Native 封装层（C++）

**职责**：
- 封装 MLLM 推理引擎
- 提供 NAPI 接口供 ArkTS 调用
- 数据格式转换（ArkTS ↔ C++）
- 内存管理和资源释放
- 异步任务调度

**技术栈**：
- C++20
- NAPI (Native API)
- MLLM Runtime

**关键组件**：
- `InferenceEngine`：推理引擎封装
- `DataConverter`：数据转换工具
- `NAPIInterface`：NAPI 接口实现

### 2.3 系统服务层

#### 2.3.1 HiAI 引擎集成层

**职责**：
- 模型编译和优化
- 硬件加速调度（NPU）
- 性能监控和调优
- 内存管理

**关键功能**：
- 模型格式转换（MLLM → HiAI）
- 图优化（算子融合、常量折叠）
- 量化支持（INT8/INT4）

#### 2.3.2 NN Runtime 集成层

**职责**：
- 计算图构建和执行
- 算子映射和调度
- 动态形状支持
- 性能分析

**关键功能**：
- MLLM IR → NN Runtime 算子映射
- CPU/NPU 混合执行
- 图分割和并行执行

### 2.4 硬件抽象层

**职责**：
- 硬件资源管理
- 设备能力检测
- 性能监控
- 功耗管理

## 3. 数据流向图

### 3.1 模型加载流程

```
ArkTS App
    │
    │ loadModel(modelPath)
    ▼
NAPI Interface
    │
    │ convertPath()
    ▼
InferenceEngine
    │
    │ loadModel()
    ▼
MLLM Runtime
    │
    │ parseModel()
    ▼
Backend Manager
    │
    ├─► HiAI Backend ──► HiAI Engine ──► NPU
    │
    └─► NN RT Backend ──► NN Runtime ──► CPU/NPU
```

### 3.2 推理执行流程

```
ArkTS App
    │
    │ inference(input)
    ▼
NAPI Interface
    │
    │ convertInput()
    ▼
InferenceEngine
    │
    │ execute()
    ▼
MLLM Runtime
    │
    │ forward()
    ▼
Backend Manager
    │
    ├─► HiAI Backend
    │   │
    │   │ compileGraph()
    │   │ executeGraph()
    │   │
    │   └─► HiAI Engine ──► NPU ──► Result
    │
    └─► NN RT Backend
        │
        │ buildGraph()
        │ executeGraph()
        │
        └─► NN Runtime ──► CPU/NPU ──► Result
            │
            └─► Result ──► InferenceEngine ──► NAPI ──► ArkTS
```

### 3.3 数据格式转换

```
ArkTS (string/image)
    │
    │ UTF-8 / PixelBuffer
    ▼
NAPI Converter
    │
    │ toCpp()
    ▼
C++ (std::string / Tensor)
    │
    │ toTensor()
    ▼
MLLM Tensor
    │
    │ toBackendFormat()
    ▼
Backend Tensor (HiAI/NN RT)
    │
    │ execute()
    ▼
Result Tensor
    │
    │ toMLLMTensor()
    ▼
MLLM Tensor
    │
    │ toCpp()
    ▼
C++ (std::string)
    │
    │ toArkTS()
    ▼
NAPI Converter
    │
    │ UTF-8
    ▼
ArkTS (string)
```

## 4. 技术选型说明

### 4.1 应用层技术选型

**ArkTS**
- **理由**：OpenHarmony 官方推荐语言，类型安全，性能优秀
- **优势**：与系统深度集成，支持声明式 UI

**ArkUI**
- **理由**：OpenHarmony 官方 UI 框架
- **优势**：跨设备适配，性能优化

### 4.2 Native 层技术选型

**C++20**
- **理由**：MLLM 框架使用 C++，性能要求高
- **优势**：零开销抽象，直接内存控制

**NAPI**
- **理由**：OpenHarmony 官方 Native API 标准
- **优势**：类型安全，自动内存管理

**MLLM Framework**
- **理由**：轻量级、高性能、支持多后端
- **优势**：已有量化、优化等能力

### 4.3 系统服务层技术选型

**HiAI Engine**
- **理由**：OpenHarmony 官方 AI 引擎，硬件加速
- **优势**：NPU 加速，性能优秀

**NN Runtime**
- **理由**：OpenHarmony 神经网络推理框架
- **优势**：统一接口，支持多设备

### 4.4 构建工具选型

**CMake**
- **理由**：跨平台，广泛支持
- **优势**：与 MLLM 构建系统一致

**DevEco Studio**
- **理由**：OpenHarmony 官方 IDE
- **优势**：完整工具链，调试方便

## 5. 架构设计原则

### 5.1 分层原则
- **职责单一**：每层只负责特定功能
- **接口清晰**：层间通过明确接口通信
- **低耦合**：层间依赖最小化

### 5.2 扩展性原则
- **后端可插拔**：支持多种 AI 引擎
- **模型可替换**：支持不同模型格式
- **设备可适配**：支持不同硬件平台

### 5.3 性能原则
- **零拷贝**：尽量减少数据拷贝
- **异步执行**：避免阻塞主线程
- **资源复用**：内存池、线程池

### 5.4 可靠性原则
- **错误处理**：完善的异常处理机制
- **资源管理**：RAII 自动资源管理
- **日志系统**：完整的日志记录

## 6. 关键技术决策

### 6.1 为什么选择 MLLM 框架？
1. **轻量级**：专为移动端设计
2. **高性能**：已有优化实现
3. **多后端**：支持 CPU/NPU/GPU
4. **量化支持**：内置量化工具

### 6.2 为什么同时集成 HiAI 和 NN Runtime？
1. **HiAI**：NPU 硬件加速，性能最优
2. **NN Runtime**：统一接口，易于扩展
3. **混合执行**：根据算子特性选择后端

### 6.3 为什么使用 NAPI？
1. **官方标准**：OpenHarmony 推荐
2. **类型安全**：编译时检查
3. **自动管理**：内存自动管理

## 7. 性能优化策略

### 7.1 模型优化
- **量化**：INT8/INT4 量化
- **剪枝**：模型剪枝
- **图优化**：算子融合、常量折叠

### 7.2 运行时优化
- **内存池**：预分配、复用
- **KV Cache**：量化 KV Cache
- **批处理**：批量推理

### 7.3 硬件优化
- **NPU 加速**：关键算子使用 NPU
- **混合执行**：CPU/NPU 混合
- **流水线**：预处理、推理、后处理并行

## 8. 安全与隐私

### 8.1 数据安全
- **本地推理**：数据不上传云端
- **内存加密**：敏感数据加密存储
- **权限控制**：最小权限原则

### 8.2 模型安全
- **模型加密**：模型文件加密
- **完整性校验**：模型签名验证
- **版本管理**：模型版本控制

---

*最后更新：2025-01-XX*



